{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a04f59-c559-47f6-b659-8b1ca7c1ee2e",
        "id": "dZiXu4eVKWUK"
      },
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 05:10:06--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat.7’\n",
            "\n",
            "\rtrain.dat.7           0%[                    ]       0  --.-KB/s               \rtrain.dat.7         100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 05:10:06 (171 MB/s) - ‘train.dat.7’ saved [11244/11244]\n",
            "\n",
            "--2023-02-18 05:10:06--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat.7’\n",
            "\n",
            "test.dat.7          100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 05:10:06 (285 MB/s) - ‘test.dat.7’ saved [2844/2844]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "b4f6bdfe-fce0-43a1-e6ab-f6983b6e7023"
      },
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = [list(map(int, instance.strip().split('\\t')))]\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += instance\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "   return sum([array2[val]*array1[val]  for val in range(len(array1))])\n",
        "\n",
        "def sigmoid(p):\n",
        "    a=math.exp(-p)+1\n",
        "    return 1 / a\n",
        "\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    val = dot_product(weight, instance)\n",
        "    z=sigmoid(val)\n",
        "    return z\n",
        "\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(w, i):\n",
        "  a=output(w, i)\n",
        "  if  a >= 0.5:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_accuracy(weights, instances):\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    # Initialize the weights to 0\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "          in_value = dot_product(weights, instance) # we calculate the input value\n",
        "          output_value = sigmoid(in_value) # we calculate the output value\n",
        "          error = instance[-1] - output_value # we calculate the error\n",
        "          for i in range(0, len(weights)): # we update the weights\n",
        "            weights[i] += lr * error * output_value * (1-output_value) * instance[i]\n",
        "    return weights\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5GQ9mRrIyEZ9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "47bea49c-7558-4275-cd46-f56e4d07547f"
      },
      "source": [
        "# Get the datasets\n",
        "!wget http://www.cse.unt.edu/~blanco/csce5218/hw1a/train.dat\n",
        "!wget http://www.cse.unt.edu/~blanco/csce5218/hw1a/test.dat"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 05:10:06--  http://www.cse.unt.edu/~blanco/csce5218/hw1a/train.dat\n",
            "Resolving www.cse.unt.edu (www.cse.unt.edu)... 129.120.151.91\n",
            "Connecting to www.cse.unt.edu (www.cse.unt.edu)|129.120.151.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K) [application/x-ns-proxy-autoconfig]\n",
            "Saving to: ‘train.dat.8’\n",
            "\n",
            "train.dat.8         100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 05:10:06 (169 MB/s) - ‘train.dat.8’ saved [11244/11244]\n",
            "\n",
            "--2023-02-18 05:10:06--  http://www.cse.unt.edu/~blanco/csce5218/hw1a/test.dat\n",
            "Resolving www.cse.unt.edu (www.cse.unt.edu)... 129.120.151.91\n",
            "Connecting to www.cse.unt.edu (www.cse.unt.edu)|129.120.151.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K) [application/x-ns-proxy-autoconfig]\n",
            "Saving to: ‘test.dat.8’\n",
            "\n",
            "test.dat.8          100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 05:10:06 (244 MB/s) - ‘test.dat.8’ saved [2844/2844]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50YvUza-BYQF",
        "outputId": "3b9dd37a-cbcb-49b2-e7f4-716c44997ec4"
      },
      "source": [
        "\n",
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "\n",
        "improv_instances_tr=[]\n",
        "st=0\n",
        "end=len(instances_tr)\n",
        "for x in range(st,end-1):\n",
        "  if(x%2==1):\n",
        "    improv_instances_tr.append(instances_tr[x])\n",
        "improv_instances_te=[]\n",
        "st=0\n",
        "end=len(instances_te)\n",
        "for x in range(st,end-1):\n",
        "  if(x%2==1):\n",
        "    improv_instances_te.append(instances_te[x])\n",
        "\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(improv_instances_tr, lr, epochs)\n",
        "\n",
        "accuracy = get_accuracy(weights, improv_instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 800, epochs:   5, learning rate: 0.005; Accuracy (test, 200 instances): 67.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the train_perceptron function, we use the dot_product and sigmoid functions because they are standard techniques for implementing a perceptron model. The dot_product function computes the weighted sum of the input features, and the resulting value is passed through the sigmoid function to generate a prediction value between 0 and 1.\n",
        "\n",
        "While the predict function you proposed could calculate the weighted sum of the input features, it doesn't include an activation function like the sigmoid function. As a result, the output of the predict function wouldn't be in the desired range of 0 to 1, which is essential for binary classification tasks.\n",
        "\n",
        "Therefore, to ensure that the perceptron produces an output in the appropriate range for binary classification, using the sigmoid function is essential."
      ],
      "metadata": {
        "id": "L9jG70LLMH3D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO Write your code below and include the output of your code.\n",
        "The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with differet hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent =  [5, 10, 25, 50, 75, 100]  # percent of the training dataset to train with\n",
        "num_epochs =[5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]         # learning rate\n",
        "\n",
        "improv_instances_tr=[]\n",
        "st=0\n",
        "end=len(instances_tr)\n",
        "for x in range(st,end-1):\n",
        "  if(x%2==1):\n",
        "    improv_instances_tr.append(instances_tr[x])\n",
        "improv_instances_te=[]\n",
        "st=0\n",
        "end=len(instances_te)\n",
        "for x in range(st,end-1):\n",
        "  if(x%2==1):\n",
        "    improv_instances_te.append(instances_te[x])\n",
        "\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(improv_instances_tr)*tr_size/100)\n",
        "      pre_instances = improv_instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, improv_instances_te)\n",
        "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(improv_instances_te)} instances): {accuracy:.1f}\")"
      ],
      "metadata": {
        "id": "JBRHtyzhNx1u",
        "outputId": "bf617955-a601-46f0-a927-02356db92d24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 68.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 70.7\n",
            "#tr: 299, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 76.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 75.8\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 69.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 77.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 65.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 68.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 75.8\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 77.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 76.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 79.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO Add your answer here (code and text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. No, it is not always necessary to train with all the training dataset to achieve the highest accuracy with the test dataset.\n",
        "\n",
        "  In fact, using all the training data may not always lead to the best performance. When the training dataset is too large, the model may overfit and memorize the training data instead of learning the underlying patterns that generalize to new, unseen data. This can result in poor performance on the test data, which is a common problem in machine learning.\n",
        "\n",
        "  Therefore, it is common practice to split the training dataset into training and validation sets, and use the validation set to monitor the model's performance during training. This helps to detect and prevent overfitting, and allows for adjusting hyperparameters and model architecture to improve performance.\n",
        "\n",
        "2. In the second case, we had a larger data set compared to the first data set but we achieved better performance in 1st case compared to the second test case. Reson of this situation is the learning rate. In the second case, even though we have a bigger data set, the learning rate is too low which results in lower performance.\n",
        "\n",
        "3. yes we can get higher accuracy more than 80% by tuning additional hyperparameters.\n",
        "\n",
        "4. Just increasing the number of epochs while maintaining the other hyperparameters would not always be advantageous. For instance, compared to the first experiment with only 20 epochs, the third trial with 100 training examples, 50 epochs, and a learning rate of 0.05 had a lower accuracy of 66%. This implies that training for additional epochs can result in overfitting and a reduction in the model's generalization capability. To enhance the performance of the model, it is essential to identify the ideal number of epochs that results in the best performance on the validation set.\n",
        "\n",
        "The perceptron algorithm's performance can be significantly impacted by choosing the right hyperparameters, such as the number of training examples, learning rate, and number of epochs."
      ],
      "metadata": {
        "id": "0Ltjz7TEPF9Q"
      }
    }
  ]
}